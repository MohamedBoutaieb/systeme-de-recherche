{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import docx\n",
    "from datetime import datetime\n",
    "import getpass  # Module for getting the username\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import shutil\n",
    "import numpy as np\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Manager qui permet de lire, classifier les fichiers ainsi que les indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager:\n",
    "    def __init__(self):\n",
    "        self.inverse_index = {}\n",
    "        self.time_index = {}\n",
    "        self.owner_index = {}\n",
    "        self.stem = PorterStemmer()\n",
    "        self.files_postings = {}\n",
    "        self.categories_postings = {}\n",
    "        self.totalDocs = 0\n",
    "        # Load the list of stop words\n",
    "        self.stopword = stopwords.words(\"english\")\n",
    "        self.get_file_postings()\n",
    "        self.get_categories_postings()\n",
    "        self.index_files_by_time()\n",
    "        self.index_files_by_owner()\n",
    "        self.fileCategory = {}\n",
    "        self.classify_files()\n",
    "        print(self.fileCategory)\n",
    "\n",
    "    def classify_files(self):\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                self.fileCategory[file] = os.path.basename(os.path.dirname(root))\n",
    "\n",
    "    def index_files_by_time(self):\n",
    "        # read the files in the docs folder\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                self.index_file_by_date(file_path)\n",
    "\n",
    "    def index_files_by_owner(self):\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                self.index_file_by_owner(file_path)\n",
    "\n",
    "    def upload_file(self, file_path):\n",
    "        full_text = \"\"\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                if file_path.endswith(\".pdf\"):\n",
    "                    pdfReader = PyPDF2.PdfFileReader(file)\n",
    "                    for pageNum in range(pdfReader.numPages):\n",
    "                        pageObj = pdfReader.getPage(pageNum)\n",
    "                        text = pageObj.extractText()\n",
    "                        full_text += text\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                full_text = file.read()\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            document = docx.Document(file_path)\n",
    "            for paragraph in document.paragraphs:\n",
    "                full_text += paragraph.text\n",
    "        return full_text\n",
    "\n",
    "    def get_categories_postings(self):\n",
    "        # read the files in the docs folder\n",
    "        folders = os.listdir(\"docs\")\n",
    "        for folder in folders:\n",
    "            self.categories_postings[folder] = {}\n",
    "            for root, dirs, files in os.walk(f\"docs/{folder}\"):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    text = self.upload_file(file_path)\n",
    "                    tokens = self.tokenize(text)\n",
    "                    for token in tokens:\n",
    "                        if token not in self.categories_postings[folder]:\n",
    "                            self.categories_postings[folder][token] = 1\n",
    "                        else:\n",
    "                            self.categories_postings[folder][token] += 1\n",
    "        return self.categories_postings\n",
    "\n",
    "    def get_file_postings(self):\n",
    "        # read the files in the docs folder\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                self.totalDocs += 1\n",
    "                file_path = os.path.join(root, file)\n",
    "                text = self.upload_file(file_path)\n",
    "                tokens = self.tokenize(text)\n",
    "                file_path = file_path.split(\"\\\\\")[-1]\n",
    "                self.files_postings[file_path] = {}\n",
    "                for token in tokens:\n",
    "                    if token not in self.files_postings[file_path]:\n",
    "                        self.files_postings[file_path][token] = 1\n",
    "                    else:\n",
    "                        self.files_postings[file_path][token] += 1\n",
    "        print(self.files_postings)\n",
    "\n",
    "    def index_file_by_date(self, file_path):\n",
    "        # Get the creation time of the file\n",
    "        timestamp = os.path.getctime(file_path)\n",
    "        file_time = datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d\")\n",
    "        # Indexing the file by date and time\n",
    "        if file_time not in self.time_index:\n",
    "            self.time_index[file_time] = set()\n",
    "        self.time_index[file_time].add(file_path)\n",
    "\n",
    "    def index_file_by_owner(self, file_path):\n",
    "        # Get the owner of the file\n",
    "        uid = os.stat(file_path).st_uid\n",
    "        # Get the owner's username\n",
    "        owner = getpass.getuser()\n",
    "        # Indexing the file by owner\n",
    "        if owner not in self.owner_index:\n",
    "            self.owner_index[owner] = set()\n",
    "        self.owner_index[owner].add(file_path)\n",
    "        return owner\n",
    "\n",
    "    def inverse_index_files(self):\n",
    "        # read the files in the docs folder\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                text = self.upload_file(file_path)\n",
    "                tokens = self.tokenize(text)\n",
    "                # inverse index the file\n",
    "                for token in tokens:\n",
    "                    if token not in self.inverse_index:\n",
    "                        self.inverse_index[token] = set()\n",
    "                    self.inverse_index[token].add(file_path.split(\"\\\\\")[-1])\n",
    "        return self.inverse_index\n",
    "\n",
    "    def search_keyword(self, keyword):\n",
    "        keyword = keyword.lower()\n",
    "        token = self.stem.stem(keyword)\n",
    "        scores = {}\n",
    "        idf = 0\n",
    "        # calculate idf\n",
    "        for keyowrd in self.inverse_index:\n",
    "            if keyowrd == token:\n",
    "                idf = np.log(self.totalDocs / (self.inverse_index[keyowrd]))\n",
    "                break\n",
    "        for file in self.files_postings:\n",
    "            if token in self.files_postings[file]:\n",
    "                tf = self.files_postings[file][token]\n",
    "                scores[file] = tf * idf\n",
    "        sorted_list = []\n",
    "        # construst a list of the files sorted by score\n",
    "        for file in scores:\n",
    "            sorted_list.append([file, scores[file]])\n",
    "        sorted_list = sorted(sorted_list, key=lambda x: x[1], reverse=True)\n",
    "        new_list = [item[0] for item in sorted_list[:2]]\n",
    "        return new_list\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        # Convert the tokens into lower case\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        # Remove the stop words and empty spaces\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in self.stopword and len(token) > 0\n",
    "        ]\n",
    "        # Remove the punctuation\n",
    "        tokens = [re.sub(r\"[^a-zA-Z0-9]\", \"\", token) for token in tokens]\n",
    "        # stem the tokens\n",
    "        # lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def word_occ_positions(self, tokens):\n",
    "        word_position = {}\n",
    "        for index, word in enumerate(tokens):\n",
    "            if word not in word_position:\n",
    "                word_position[word] = []\n",
    "            word_position[word].append(index)\n",
    "        return word_position\n",
    "\n",
    "    def search_keyword_association(self, keywords):\n",
    "        # Get the files that contain the keywords\n",
    "        fileScores = {}\n",
    "\n",
    "        tokens = self.tokenize(keywords)\n",
    "        word_occ_positions = self.word_occ_positions(tokens)\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                fileScores[file] = 0\n",
    "                self.fileCategory[file] = os.path.basename(os.path.dirname(root))\n",
    "        for file in self.files_postings:\n",
    "            for keyword in word_occ_positions:\n",
    "                if keyword in self.files_postings[file]:\n",
    "                    fileScores[file] += (\n",
    "                        len(word_occ_positions[keyword])\n",
    "                        * self.files_postings[file][keyword]\n",
    "                    )\n",
    "        sorted_list = []\n",
    "        # construst a list of the files sorted by score\n",
    "        for file in fileScores:\n",
    "            sorted_list.append([file, fileScores[file]])\n",
    "        sorted_list = sorted(sorted_list, key=lambda x: x[1], reverse=True)\n",
    "        new_list = [item[0] for item in sorted_list[:2]]\n",
    "        return new_list\n",
    "\n",
    "    def classify_file(self, file_path):\n",
    "        uploaded_file = self.upload_file(file_path)\n",
    "        tokens = self.tokenize(uploaded_file)\n",
    "        word_occ_positions = self.word_occ_positions(tokens)\n",
    "        max_section = self.evaluate_text(word_occ_positions)\n",
    "        \n",
    "        file = os.path.basename(file_path)\n",
    "        shutil.copy2(file_path, f\"docs/{max_section}/{file.split('.')[-1]}/{file}\")\n",
    "\n",
    "    def evaluate_text(self, word_occ_positions):\n",
    "        # formule de classification : score = (tf=n) * idf=(no) * 1\n",
    "        scores = {}\n",
    "        max_score = 0\n",
    "        max_section = \"\"\n",
    "        for section in self.categories_postings:\n",
    "            scores[section] = 0\n",
    "            for keyword in self.categories_postings[section]:\n",
    "                if keyword in word_occ_positions:\n",
    "                    scores[section] += (\n",
    "                        len(word_occ_positions[keyword])\n",
    "                        * self.categories_postings[section][keyword]\n",
    "                    )\n",
    "            if scores[section] > max_score:\n",
    "                max_score = scores[section]\n",
    "                max_section = section\n",
    "        # print(scores)\n",
    "        return max_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cookingDoc.txt': {'word': 107, 'cook': 749, 'recip': 214, 'ingredi': 107, 'cuisin': 214, 'chef': 107, 'food': 963, 'flavor': 107, 'season': 107, 'spice': 107, 'herb': 107, 'bake': 107, 'grill': 107, 'roast': 107, 'fri': 107, 'saut': 107, 'simmer': 107, 'boil': 107, 'steam': 107, 'chop': 107, 'slice': 107, 'dice': 107, 'mix': 107, 'blend': 107, 'whisk': 107, 'stir': 107, 'knead': 107, 'measur': 107, 'tast': 214, 'prepar': 107, 'marin': 107, 'sear': 107, 'broil': 107, 'barbecu': 107, 'oven': 107, 'stove': 107, 'pan': 107, 'pot': 107, 'knife': 107, 'cut': 107, 'board': 107, 'utensil': 214, 'cookwar': 107, 'cookbook': 107, 'menu': 214, 'appet': 107, 'main': 107, 'cours': 107, 'dessert': 107, 'salad': 107, 'soup': 107, 'sauc': 107, 'marinad': 107, 'garnish': 107, 'present': 214, 'plate': 107, 'techniqu': 107, 'culinari': 321, 'homemad': 107, 'gourmet': 107, 'healthi': 107, 'nutriti': 107, 'comfort': 107, 'intern': 107, 'region': 107, 'dish': 107, 'show': 107, 'network': 107, 'foodi': 107, 'eat': 107, 'school': 107, 'pair': 107, 'meal': 214, 'famili': 107, 'dinner': 107, 'celebr': 107, 'gastronomi': 107, 'art': 107, 'farmtot': 107, 'fresh': 107, 'local': 107, 'organ': 107, 'vegetarian': 107, 'vegan': 107, 'protein': 107, 'carbohydr': 107, 'fat': 107, 'diet': 107, 'cultur': 107, 'photographi': 107, 'blogger': 107, 'critic': 107, 'class': 107, 'challeng': 107, 'prep': 107, 'plan': 107, 'develop': 107, 'contest': 107, 'pastri': 107, 'grain': 107, 'dough': 107, 'sourdough': 107, 'ferment': 106, 'fermentat': 1, 'ion': 1}, 'sportsDoc.txt': {'word': 109, 'run': 218, 'footbal': 109, 'basketbal': 109, 'tenni': 109, 'swim': 109, 'soccer': 109, 'basebal': 109, 'cycl': 109, 'golf': 109, 'athlet': 218, 'volleybal': 109, 'badminton': 109, 'box': 109, 'rugbi': 109, 'cricket': 109, 'skate': 109, 'ski': 109, 'snowboard': 109, 'surf': 109, 'hike': 109, 'marathon': 109, 'triathlon': 109, 'train': 218, 'exercis': 109, 'fit': 218, 'competit': 109, 'olymp': 109, 'world': 109, 'championship': 218, 'tournament': 109, 'match': 109, 'score': 109, 'team': 109, 'player': 109, 'goal': 109, 'scoreboard': 109, 'refere': 109, 'coach': 109, 'uniform': 109, 'jersey': 109, 'equip': 109, 'ball': 109, 'racket': 109, 'bat': 109, 'net': 109, 'stadium': 109, 'arena': 109, 'track': 109, 'field': 109, 'court': 109, 'pitch': 109, 'dribbl': 109, 'shoot': 109, 'pass': 109, 'goalkeep': 109, 'touchdown': 109, 'home': 109, 'ace': 109, 'forehand': 109, 'backhand': 109, 'par': 109, 'birdi': 109, 'punch': 109, 'kick': 109, 'sprint': 109, 'endur': 109, 'strength': 109, 'speed': 109, 'strategi': 109, 'tactic': 109, 'win': 109, 'lose': 109, 'champion': 109, 'medal': 109, 'podium': 109, 'celebr': 109, 'support': 109, 'fan': 109, 'cheer': 109, 'sportsmanship': 109, 'fair': 109, 'play': 109, 'injuri': 109, 'recoveri': 109, 'warmup': 109, 'cooldown': 109, 'teamwork': 109, 'solo': 109, 'record': 109, 'level': 109, 'cup': 109, 'leagu': 109, 'halftim': 109, 'fulltim': 109, 'quarter': 109, 'inning': 109, 'tiebreak': 109, 'camp': 109}, 'supposed_sports.txt': {'nassau': 1, '': 78, 'bahama': 1, 'last': 1, 'decad': 1, 'tiger': 8, 'wood': 11, 'career': 2, 'seri': 1, 'stop': 1, 'start': 1, 'golf': 6, 'world': 4, 'grasp': 1, 'everi': 2, 'comeback': 2, 'goe': 1, 'away': 1, 'promis': 1, 'anoth': 1, 'new': 3, 'begin': 1, 'brought': 1, 'group': 1, 'wait': 2, 'patient': 1, 'mercedesbenz': 1, 'much': 1, 'like': 3, 'one': 1, 'ferri': 1, 'key': 1, 'partner': 1, 'part': 1, 'albani': 1, 'cours': 2, 'appear': 1, 'seemingli': 1, 'thin': 1, 'air': 1, 'walk': 2, 'around': 2, 'corner': 1, 'white': 1, 'tent': 1, 'alon': 1, 'simpli': 1, 'morn': 1, 'stroll': 1, 'said': 1, 'hey': 1, 'guy': 1, 'media': 1, '1': 1, 'player': 2, 'field': 1, 'rel': 1, 'obscur': 1, 'tournament': 1, 'caribbean': 1, '2023': 2, 'pga': 6, 'tour': 6, 'champion': 2, 'two': 1, 'major': 3, 'winner': 1, 'rest': 1, 'biggest': 1, 'name': 1, 'alway': 1, 'attent': 1, 'focus': 1, 'instead': 2, 'man': 1, 'current': 1, 'rank': 1, '1328': 1, 'first': 1, 'sat': 1, 'annual': 1, 'press': 1, 'confer': 1, 'preview': 1, 'hero': 1, 'challeng': 1, 'nocut': 1, 'limitedfield': 1, 'event': 1, 'host': 1, 'buddi': 1, 'discuss': 1, 'state': 1, 'look': 1, '15time': 1, 'convers': 1, 'unfold': 1, 'realiti': 1, 'person': 1, 'front': 1, 'us': 1, 'becam': 1, 'clear': 1, 'polici': 1, 'board': 1, 'member': 1, 'cofound': 1, 'leagu': 1, 'investor': 1, 'restaurateur': 1, 'design': 1, '47yearold': 1, 'legend': 1, 'transit': 1, 'toward': 1, 'becom': 1, 'authorit': 1, 'senior': 2, 'presenc': 1, 'sport': 1, 'crisi': 1, 'say': 4, 'quip': 1, 'yet': 1, 'got': 1, 'coupl': 1, 'year': 1, 'also': 1, 'golfer': 1, 'plan': 1, 'keep': 1, 'play': 3, 'right': 1, 'ankl': 1, 'strong': 1, 'enough': 1, 'allow': 1, '18': 1, 'hole': 1, 'without': 2, 'pain': 1, 'follow': 1, 'postmast': 1, 'subtalar': 1, 'fusion': 1, 'surgeri': 1, 'even': 1, 'hint': 1, 'onceamonth': 1, 'schedul': 1, '2024': 1, 'would': 1, 'includ': 1, 'four': 1, 'littl': 1, 'focu': 2, 'tuesday': 1, 'abil': 1, 'futur': 2, 'joke': 1, 'curiou': 1, 'go': 1, 'happen': 2, 'done': 1, 'while': 1, 'latest': 1, 'secondari': 1, 'storylin': 1, 'men': 1, 'profession': 1, 'politicianexecut': 1, 'talk': 1, 'confid': 1, 'issu': 1, 'answer': 1, 'question': 1, 'statu': 1, 'public': 1, 'invest': 1, 'fund': 1, 'saudi': 1, 'arabia': 1, 'negoti': 1, 'convict': 1, 'commission': 1, 'jay': 1, 'monahan': 2, 'mayb': 1, 'author': 1, 'wield': 1, 'power': 1, 'three': 1, 'differ': 1, 'time': 1, 'make': 1, 'deal': 1, 'pif': 1, 'input': 1}, 'modern_travel.docx': {'sustain': 2, 'emerg': 1, 'paramount': 1, 'consider': 1, 'modern': 1, 'travel': 2, '': 6, 'consciou': 1, 'strive': 1, 'minim': 1, 'environment': 2, 'footprint': 1, 'opt': 1, 'ecofriendli': 1, 'accommod': 1, 'support': 1, 'local': 1, 'initi': 1, 'choos': 1, 'transport': 1, 'option': 1, 'priorit': 1, 'respons': 1, 'tourism': 1, 'trend': 1, 'commit': 1, 'preserv': 1, 'beauti': 1, 'destin': 1, 'futur': 1, 'generationsth': 1, 'concept': 1, 'time': 1}, 'file2.txt': {'titl': 1, '': 80, 'wanderlust': 1, 'unleash': 1, 'transform': 3, 'power': 2, 'travel': 18, 'introduct': 1, 'movement': 1, 'one': 4, 'place': 3, 'anoth': 2, 's': 6, 'journey': 6, 'expand': 1, 'horizon': 1, 'enrich': 2, 'soul': 1, 'broaden': 1, 'perspect': 1, 'allur': 1, 'discov': 1, 'new': 1, 'landscap': 3, 'immers': 3, 'oneself': 3, 'differ': 2, 'cultur': 5, 'embrac': 3, 'unknown': 3, 'fuel': 1, 'human': 2, 'spirit': 1, 'centuri': 1, 'articl': 1, 'explor': 6, 'invit': 2, 'embark': 4, 'goe': 1, 'beyond': 2, 'mile': 1, 'map': 1, 'art': 2, 'form': 2, 'us': 6, 'vast': 3, 'tapestri': 3, 'world': 5, 'bustl': 2, 'market': 2, 'marrakech': 1, 'seren': 3, 'patagonia': 1, 'destin': 1, 'chapter': 1, 'wait': 2, 'read': 1, 'thrill': 1, 'discoveri': 1, 'lie': 1, 'famou': 1, 'landmark': 1, 'also': 2, 'hidden': 1, 'gem': 1, 'local': 2, 'unchart': 1, 'corner': 1, 'reveal': 1, 'true': 1, 'essenc': 1, 'aspect': 1, 'opportun': 2, 'divers': 2, 'whether': 4, 'savor': 1, 'street': 1, 'food': 1, 'bangkok': 1, 'wit': 2, 'tradit': 1, 'tea': 1, 'ceremoni': 1, 'kyoto': 1, 'danc': 1, 'rhythm': 1, 'latin': 1, 'america': 1, 'everi': 2, 'leav': 2, 'indel': 2, 'mark': 2, 'foster': 2, 'understand': 1, 'intricaci': 1, 'celebr': 1, 'make': 1, 'uniqu': 1, 'common': 2, 'thread': 2, 'connect': 4, 'person': 3, 'growth': 3, 'adventur': 2, 'challeng': 1, 'step': 1, 'comfort': 1, 'zone': 1, 'navig': 2, 'bazaar': 1, 'conquer': 1, 'mountain': 2, 'peak': 1, 'maze': 1, 'foreign': 1, 'languag': 1, 'becom': 4, 'lesson': 1, 'resili': 1, 'adapt': 1, 'stem': 1, 'experi': 4, 'last': 1, 'souvenir': 1, 'accompani': 1, 'long': 1, 'end': 1, 'natur': 4, 'symphoni': 1, 'masterpiec': 1, 'wonder': 2, 'allow': 1, 'grandeur': 1, 'firsthand': 1, 'aweinspir': 1, 'majesti': 1, 'niagara': 1, 'fall': 1, 'beauti': 2, 'swiss': 1, 'alp': 1, 'teacher': 1, 'inspir': 1, 'awe': 1, 'rever': 1, 'offer': 2, 'chanc': 1, 'reconnect': 1, 'earth': 1, 'sens': 1, 'respons': 1, 'toward': 1, 'preserv': 1, 'friendship': 2, 'magic': 1, 'way': 1, 'forg': 1, 'fellow': 1, 'wander': 1, 'share': 3, 'stori': 2, 'hostel': 1, 'room': 1, 'group': 1, 'trek': 1, 'strike': 1, 'convers': 1, 'bond': 1, 'often': 1, 'transcend': 2, 'border': 1, 'cultiv': 1, 'road': 1, 'testament': 1, 'global': 1, 'kinship': 1, 'reflect': 2, 'selfdiscoveri': 2, 'physic': 2, 'inner': 2, 'provid': 1, 'moment': 1, 'solitud': 1, 'reassess': 1, 'rediscov': 1, 'quiet': 1, 'sunris': 1, 'beach': 1, 'contempl': 1, 'walk': 1, 'ancient': 1, 'ruin': 1, 'still': 1, 'summital': 1, 'introspect': 1, 'creed': 2, 'storytel': 1, 'collect': 1, 'memori': 1, 'shape': 1, 'narr': 1, 'curios': 1, 'open': 1, 'willing': 1, 'acknowledg': 1, 'short': 1, 'weekend': 1, 'getaway': 1, 'monthslong': 1, 'expedit': 1, 'contribut': 1, 'everevolv': 1, 'live': 1, 'conclus': 1, 'profound': 2, 'mere': 1, 'act': 1, 'move': 1, 'weav': 1, 'togeth': 1, 'joy': 1, 'travers': 1, 'journeyon': 1, 'heart': 1, 'mind': 1, 'remind': 1, 'wondrou': 1}, 'supposed_travel.txt': {'travel': 3, 'transform': 1, 'journey': 1, 'extend': 1, 'beyond': 1, 'physic': 1, 'act': 1, 'move': 1, 'one': 2, 'place': 1, 'anoth': 1, '': 19, 'explor': 2, 'cultur': 1, 'landscap': 1, 'oneself': 1, 'whether': 1, 'ventur': 1, 'bustl': 1, 'citi': 1, 'seren': 1, 'natur': 1, 'wonder': 1, 'remot': 1, 'corner': 1, 'globe': 1, 'provid': 1, 'canva': 1, 'new': 1, 'experi': 1, 'perspect': 1, 'tapestri': 2, 'woven': 1, 'thread': 1, 'discoveri': 1, 'foster': 1, 'deep': 1, 'appreci': 1, 'divers': 1, 'world': 1, 'encount': 1, 'differ': 1, 'custom': 1, 'cuisin': 1, 'tradit': 1, 'becom': 1, 'catalyst': 1, 'person': 1, 'growth': 1, 'sourc': 1, 'cherish': 1, 'memori': 1, 'adventur': 1, 'matter': 1, 'near': 1, 'far': 1, 'offer': 1, 'opportun': 1, 'step': 1, 'outsid': 1, 's': 1, 'comfort': 1, 'zone': 1, 'embrac': 1, 'unknown': 1, 'celebr': 1, 'rich': 1, 'come': 1, 'vast': 1, 'share': 1, 'human': 1}, 'travelDoc.txt': {'word': 1, 'travel': 1284, 'adventur': 321, 'explor': 214, 'journey': 107, 'destin': 107, 'vacat': 214, 'tourism': 107, 'wanderlust': 107, 'backpack': 107, 'hike': 214, 'camp': 107, 'sightse': 107, 'discov': 107, 'exot': 107, 'itinerari': 107, 'map': 107, 'guidebook': 107, 'passport': 107, 'visa': 107, 'airport': 107, 'airlin': 107, 'flight': 107, 'hotel': 107, 'accommod': 107, 'hostel': 107, 'resort': 107, 'cruis': 214, 'car': 107, 'rental': 107, 'road': 107, 'trip': 107, 'train': 107, 'cultur': 214, 'experi': 107, 'local': 214, 'cuisin': 107, 'street': 107, 'food': 107, 'landmark': 107, 'monument': 107, 'museum': 107, 'galleri': 107, 'natur': 107, 'wildlif': 107, 'beach': 107, 'mountain': 107, 'desert': 107, 'forest': 107, 'island': 107, 'countrysid': 107, 'cityscap': 107, 'sunset': 107, 'sunris': 107, 'photographi': 107, 'sport': 107, 'scuba': 107, 'dive': 107, 'snorkel': 107, 'surf': 107, 'ski': 107, 'paraglid': 107, 'hot': 107, 'air': 107, 'balloon': 107, 'trail': 107, 'trek': 107, 'cycl': 107, 'nomad': 214, 'blogger': 107, 'vlog': 107, 'solo': 107, 'group': 107, 'famili': 107, 'digit': 107, 'insur': 107, 'safeti': 107, 'currenc': 107, 'exchang': 107, 'languag': 107, 'barrier': 107, 'custom': 107, 'tradit': 107, 'peopl': 107, 'hospit': 107, 'divers': 107, 'histor': 107, 'site': 107, 'archaeolog': 107, 'ruin': 107, 'landscap': 107, 'climat': 107, 'weather': 107, 'pack': 107, 'souvenir': 107, 'expens': 107, 'budget': 107, 'luxuri': 107, 'agenc': 107, 'tour': 107, 'oper': 107, 'ship': 107, 'passeng': 107, 'navig': 107, 'relax': 107, 'memori': 107, 'scenic': 107, 'view': 107, 'plan': 107, 'commun': 107, 'ecotour': 107, 'global': 107, 'remot': 107, 'place': 107, 'visit': 107, 'beaten': 107, 'path': 107, 'cityword': 106, 'citi': 1}}\n",
      "{'cookingDoc.txt': 'cooking', 'sportsDoc.txt': 'sports', 'supposed_sports.txt': 'sports', 'modern_travel.docx': 'travel', 'file2.txt': 'travel', 'supposed_travel.txt': 'travel', 'travelDoc.txt': 'travel'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemple d'utilisation\n",
    "file_manager = FileManager()\n",
    "\n",
    "\n",
    "with open(\"time_index.txt\", \"w\") as file:\n",
    "    for time in file_manager.time_index:\n",
    "        file.write(f\"{time} {file_manager.time_index[time]}\\n\")\n",
    "\n",
    "# Write owner_index into a file\n",
    "with open(\"owner_index.txt\", \"w\") as file:\n",
    "    for owner in file_manager.owner_index:\n",
    "        file.write(f\"{owner} {file_manager.owner_index[owner]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de nouveaux documents dans notre arborescene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cette fonction determine la categorie du fichier et le classifie dans l'arborecence des dossiers\n",
    "\n",
    "file = input(\"Enter the file name: \")\n",
    "file_manager.classify_file(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation du système de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notre precision de classification 0.9333333333333333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_sports</th>\n",
       "      <th>pred_cooking</th>\n",
       "      <th>pred_travel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cooking</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred_sports  pred_cooking  pred_travel\n",
       "sports            13             0            2\n",
       "cooking            0            14            0\n",
       "travel             0             1           15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def read_texts_from_csv(csv_file):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        for row in csv_reader:\n",
    "            text = row['Text']\n",
    "            label = row['Category'].lower()\n",
    "            labels.append(label)\n",
    "            texts.append(text)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "csv_file_path = 'dataset.csv'\n",
    "\n",
    "# Read texts from the CSV dataset\n",
    "texts_without_labels,labels = read_texts_from_csv(csv_file_path)\n",
    "\n",
    "predictions = []\n",
    "for text in texts_without_labels:\n",
    "    tokens = file_manager.tokenize(text)\n",
    "    word_occ_positions = file_manager.word_occ_positions(tokens)\n",
    "    max_section = file_manager.evaluate_text(word_occ_positions)\n",
    "    predictions.append(max_section)\n",
    "\n",
    "score = 0\n",
    "# Step 6: Evaluate the accuracy of the model\n",
    "for i in range(0,len(labels)):\n",
    "    if labels[i] == predictions[i]:\n",
    "        score+=1\n",
    "score /= len(labels) \n",
    "print(f'notre precision de classification {score}')\n",
    "# draw confusion matrix \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "array = confusion_matrix(labels, predictions, labels=['sports', 'cooking', 'travel'])\n",
    "df_cm = pd.DataFrame(array, index = ['sports', 'cooking', 'travel'],\n",
    "                  columns = ['pred_sports', 'pred_cooking', 'pred_travel'])\n",
    "\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche de Fichiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 results:\n",
      "supposed_travel.txt\n",
      "travelDoc.txt\n"
     ]
    }
   ],
   "source": [
    "def search_by_keyword():\n",
    "    keyword = input(\"Enter the keyword: \")\n",
    "    word =stemmer.stem(keyword.lower())\n",
    "    results = file_manager.search_keyword(word)\n",
    "    if len(results) == 0:\n",
    "        print(\"No results found\")\n",
    "    else:\n",
    "        print(f\"Found {len(results)} results:\")\n",
    "        for result in results:\n",
    "            print(result)\n",
    "\n",
    "search_by_keyword()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 results:\n",
      "travelDoc.txt\n",
      "file2.txt\n"
     ]
    }
   ],
   "source": [
    "def search_by_keywords_association():\n",
    "    keywords = input(\"Enter the keywords :\")\n",
    "    results = file_manager.search_keyword_association(keywords)\n",
    "    if len(results) == 0:\n",
    "        print(\"No results found\")\n",
    "    else:\n",
    "        print(f\"Found {len(results)} results:\")\n",
    "        for result in results:\n",
    "            print(result)\n",
    "\n",
    "search_by_keywords_association()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modern_travel.docx',\n",
       " 'file2.txt',\n",
       " 'supposed_travel.txt',\n",
       " 'supposé_travel.txt',\n",
       " 'travelDoc.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_in_content():\n",
    "    keyword = input(\"Enter the keyword: \")\n",
    "    results = []\n",
    "    for root, dirs, files in os.walk('docs'):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                file_content = file_manager.upload_file(file_path)\n",
    "                if keyword.lower() in file_content.lower():\n",
    "                        results.append(file_path.split(\"\\\\\")[-1])\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la lecture du fichier {file_path}: {e}\")\n",
    "    return results\n",
    "search_in_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation des critères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notre precision de recherche 0.85\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_sports</th>\n",
       "      <th>pred_cooking</th>\n",
       "      <th>pred_travel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cooking</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred_sports  pred_cooking  pred_travel\n",
       "sports             5             0            2\n",
       "cooking            0             5            1\n",
       "travel             1             0            6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path = \"dataset_for_query.csv\"\n",
    "\n",
    "# Read texts from the CSV dataset\n",
    "texts_without_labels, labels = read_texts_from_csv(csv_file_path)\n",
    "\n",
    "score = 0\n",
    "predictions = []\n",
    "i = 0\n",
    "for text in texts_without_labels:\n",
    "    max_sections = file_manager.search_keyword_association(text)\n",
    "    local_score = 0\n",
    "    for section in max_sections:\n",
    "        if file_manager.fileCategory[section] == labels[i]:\n",
    "            local_score += 1\n",
    "    local_score = local_score / len(max_sections)\n",
    "    if local_score >= 0.5:\n",
    "        score += 1\n",
    "    predictions.append(file_manager.fileCategory[max_sections[0]])\n",
    "    i += 1\n",
    "\n",
    "score = score / len(labels)\n",
    "print(f\"notre precision de recherche {score}\")\n",
    "# draw confusion matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "array = confusion_matrix(labels, predictions, labels=[\"sports\", \"cooking\", \"travel\"])\n",
    "df_cm = pd.DataFrame(\n",
    "    array,\n",
    "    index=[\"sports\", \"cooking\", \"travel\"],\n",
    "    columns=[\"pred_sports\", \"pred_cooking\", \"pred_travel\"],\n",
    ")\n",
    "\n",
    "df_cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
