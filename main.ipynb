{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import docx\n",
    "from datetime import datetime\n",
    "import getpass  # Module for getting the username\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import shutil\n",
    "import numpy as np\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Manager qui permet de lire, classifier les fichiers ainsi que les indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager:\n",
    "    def __init__(self):\n",
    "        self.inverse_index = {}\n",
    "        self.time_index = {}\n",
    "        self.owner_index = {}\n",
    "        self.stem = PorterStemmer()\n",
    "        self.files_postings = {}\n",
    "        self.categories_postings = {}\n",
    "        self.totalDocs = 0\n",
    "        # Load the list of stop words\n",
    "        self.stopword = stopwords.words(\"english\")\n",
    "        self.get_file_postings()\n",
    "        self.get_categories_postings()\n",
    "        self.index_files_by_time()\n",
    "        self.index_files_by_owner()\n",
    "        self.fileCategory = {}\n",
    "        self.classify_files()\n",
    "        print(self.fileCategory)\n",
    "\n",
    "    def classify_files(self):\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                self.fileCategory[file] = os.path.basename(os.path.dirname(root))\n",
    "\n",
    "    def index_files_by_time(self):\n",
    "        # read the files in the docs folder\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                self.index_file_by_date(file_path)\n",
    "\n",
    "    def index_files_by_owner(self):\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                self.index_file_by_owner(file_path)\n",
    "\n",
    "    def upload_file(self, file_path):\n",
    "        full_text = \"\"\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                if file_path.endswith(\".pdf\"):\n",
    "                    pdfReader = PyPDF2.PdfFileReader(file)\n",
    "                    for pageNum in range(pdfReader.numPages):\n",
    "                        pageObj = pdfReader.getPage(pageNum)\n",
    "                        text = pageObj.extractText()\n",
    "                        full_text += text\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                full_text = file.read()\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            document = docx.Document(file_path)\n",
    "            for paragraph in document.paragraphs:\n",
    "                full_text += paragraph.text\n",
    "        return full_text\n",
    "\n",
    "    def get_categories_postings(self):\n",
    "        # read the files in the docs folder\n",
    "        folders = os.listdir(\"docs\")\n",
    "        for folder in folders:\n",
    "            self.categories_postings[folder] = {}\n",
    "            for root, dirs, files in os.walk(f\"docs/{folder}\"):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    text = self.upload_file(file_path)\n",
    "                    tokens = self.tokenize(text)\n",
    "                    for token in tokens:\n",
    "                        if token not in self.categories_postings[folder]:\n",
    "                            self.categories_postings[folder][token] = 1\n",
    "                        else:\n",
    "                            self.categories_postings[folder][token] += 1\n",
    "        return self.categories_postings\n",
    "\n",
    "    def get_file_postings(self):\n",
    "        # read the files in the docs folder\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                self.totalDocs += 1\n",
    "                file_path = os.path.join(root, file)\n",
    "                text = self.upload_file(file_path)\n",
    "                tokens = self.tokenize(text)\n",
    "                file_path = file_path.split(\"\\\\\")[-1]\n",
    "                self.files_postings[file_path] = {}\n",
    "                for token in tokens:\n",
    "                    if token not in self.files_postings[file_path]:\n",
    "                        self.files_postings[file_path][token] = 1\n",
    "                    else:\n",
    "                        self.files_postings[file_path][token] += 1\n",
    "\n",
    "    def index_file_by_date(self, file_path):\n",
    "        # Get the creation time of the file\n",
    "        timestamp = os.path.getctime(file_path)\n",
    "        file_time = datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d\")\n",
    "        # Indexing the file by date and time\n",
    "        if file_time not in self.time_index:\n",
    "            self.time_index[file_time] = set()\n",
    "        self.time_index[file_time].add(file_path)\n",
    "\n",
    "    def index_file_by_owner(self, file_path):\n",
    "        # Get the owner of the file\n",
    "        uid = os.stat(file_path).st_uid\n",
    "        # Get the owner's username\n",
    "        owner = getpass.getuser()\n",
    "        # Indexing the file by owner\n",
    "        if owner not in self.owner_index:\n",
    "            self.owner_index[owner] = set()\n",
    "        self.owner_index[owner].add(file_path)\n",
    "        return owner\n",
    "\n",
    "    def inverse_index_files(self):\n",
    "        # read the files in the docs folder\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                text = self.upload_file(file_path)\n",
    "                tokens = self.tokenize(text)\n",
    "                # inverse index the file\n",
    "                for token in tokens:\n",
    "                    if token not in self.inverse_index:\n",
    "                        self.inverse_index[token] = set()\n",
    "                    self.inverse_index[token].add(file_path.split(\"\\\\\")[-1])\n",
    "        return self.inverse_index\n",
    "\n",
    "    def search_keyword(self, keyword):\n",
    "        keyword = keyword.lower()\n",
    "        token = self.stem.stem(keyword)\n",
    "        scores = {}\n",
    "        idf = 0\n",
    "        # calculate idf\n",
    "        for keyowrd in self.inverse_index:\n",
    "            if keyowrd == token:\n",
    "                idf = np.log(self.totalDocs / (self.inverse_index[keyowrd]))\n",
    "                break\n",
    "        for file in self.files_postings:\n",
    "            if token in self.files_postings[file]:\n",
    "                tf = self.files_postings[file][token]\n",
    "                scores[file] = tf * idf\n",
    "        sorted_list = []\n",
    "        # construst a list of the files sorted by score\n",
    "        for file in scores:\n",
    "            sorted_list.append([file, scores[file]])\n",
    "        sorted_list = sorted(sorted_list, key=lambda x: x[1], reverse=True)\n",
    "        new_list = [item[0] for item in sorted_list[:2]]\n",
    "        return new_list\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        # Convert the tokens into lower case\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        # Remove the stop words and empty spaces\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in self.stopword and len(token) > 0\n",
    "        ]\n",
    "        # Remove the punctuation\n",
    "        tokens = [re.sub(r\"[^a-zA-Z0-9]\", \"\", token) for token in tokens]\n",
    "        # stem the tokens\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def word_occ_positions(self, tokens):\n",
    "        word_position = {}\n",
    "        for index, word in enumerate(tokens):\n",
    "            if word not in word_position:\n",
    "                word_position[word] = []\n",
    "            word_position[word].append(index)\n",
    "        return word_position\n",
    "\n",
    "    def search_keyword_association(self, keywords):\n",
    "        # Get the files that contain the keywords\n",
    "        fileScores = {}\n",
    "\n",
    "        tokens = self.tokenize(keywords)\n",
    "        word_occ_positions = self.word_occ_positions(tokens)\n",
    "        for root, dirs, files in os.walk(\"docs\"):\n",
    "            for file in files:\n",
    "                fileScores[file] = 0\n",
    "                self.fileCategory[file] = os.path.basename(os.path.dirname(root))\n",
    "        for file in self.files_postings:\n",
    "            for keyword in word_occ_positions:\n",
    "                if keyword in self.files_postings[file]:\n",
    "                    fileScores[file] += (\n",
    "                        len(word_occ_positions[keyword])\n",
    "                        * self.files_postings[file][keyword]\n",
    "                    )\n",
    "        sorted_list = []\n",
    "        # construst a list of the files sorted by score\n",
    "        for file in fileScores:\n",
    "            sorted_list.append([file, fileScores[file]])\n",
    "        sorted_list = sorted(sorted_list, key=lambda x: x[1], reverse=True)\n",
    "        new_list = [item[0] for item in sorted_list[:2]]\n",
    "        return new_list\n",
    "\n",
    "    def classify_file(self, file_path):\n",
    "        uploaded_file = self.upload_file(file_path)\n",
    "        tokens = self.tokenize(uploaded_file)\n",
    "        word_occ_positions = self.word_occ_positions(tokens)\n",
    "        max_section = self.evaluate_text(word_occ_positions)\n",
    "        # if (max_section ==''):\n",
    "        #     rand = ['sports','cooking','travel']\n",
    "        #     max_section = rand[random.randint(0,2)]\n",
    "        file = os.path.basename(file_path)\n",
    "        shutil.copy2(file_path, f\"docs/{max_section}/{file.split('.')[1]}/{file}\")\n",
    "\n",
    "    def evaluate_text(self, word_occ_positions):\n",
    "        # formule de classification : score = tf=n * idf=(no) * 1\n",
    "        scores = {}\n",
    "        max_score = 0\n",
    "        max_section = \"\"\n",
    "        for section in self.categories_postings:\n",
    "            scores[section] = 0\n",
    "            for keyword in self.categories_postings[section]:\n",
    "                if keyword in word_occ_positions:\n",
    "                    scores[section] += (\n",
    "                        len(word_occ_positions[keyword])\n",
    "                        * self.categories_postings[section][keyword]\n",
    "                    )\n",
    "\n",
    "            if scores[section] > max_score:\n",
    "                max_score = scores[section]\n",
    "                max_section = section\n",
    "        # print(scores)\n",
    "        return max_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cookingDoc.txt': 'cooking', 'sportsDoc.txt': 'sports', 'supposed_sports.txt': 'sports', 'modern_travel.docx': 'travel', 'file2.txt': 'travel', 'supposed_travel.txt': 'travel', 'travelDoc.txt': 'travel'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemple d'utilisation\n",
    "file_manager = FileManager()\n",
    "\n",
    "test = file_manager.upload_file(\"gaming.txt\")\n",
    "\n",
    "tokens = file_manager.tokenize(test)\n",
    "\n",
    "# write time_index into a  file\n",
    "with open(\"time_index.txt\", \"w\") as file:\n",
    "    for time in file_manager.time_index:\n",
    "        file.write(f\"{time} {file_manager.time_index[time]}\\n\")\n",
    "\n",
    "# Write owner_index into a file\n",
    "with open(\"owner_index.txt\", \"w\") as file:\n",
    "    for owner in file_manager.owner_index:\n",
    "        file.write(f\"{owner} {file_manager.owner_index[owner]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de nouveaux documents dans notre arborescene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cette fonction determine la categorie du fichier et le classifie dans l'arborecence des dossiers\n",
    "\n",
    "file = input(\"Enter the file name: \")\n",
    "file_manager.classify_file(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation du système de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notre precision de classification 0.8333333333333334\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_sports</th>\n",
       "      <th>pred_cooking</th>\n",
       "      <th>pred_travel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cooking</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred_sports  pred_cooking  pred_travel\n",
       "sports            19             0            9\n",
       "cooking            0            24            3\n",
       "travel             0             2           27"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def read_texts_from_csv(csv_file):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        for row in csv_reader:\n",
    "            text = row['Text']\n",
    "            label = row['Category'].lower()\n",
    "            labels.append(label)\n",
    "            texts.append(text)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "csv_file_path = 'dataset.csv'\n",
    "\n",
    "# Read texts from the CSV dataset\n",
    "texts_without_labels,labels = read_texts_from_csv(csv_file_path)\n",
    "\n",
    "predictions = []\n",
    "for text in texts_without_labels:\n",
    "    tokens = file_manager.tokenize(text)\n",
    "    word_occ_positions = file_manager.word_occ_positions(tokens)\n",
    "    max_section = file_manager.evaluate_text(word_occ_positions)\n",
    "    predictions.append(max_section)\n",
    "\n",
    "score = 0\n",
    "# Step 6: Evaluate the accuracy of the model\n",
    "for i in range(0,len(labels)):\n",
    "    if labels[i] == predictions[i]:\n",
    "        score+=1\n",
    "score /= len(labels) \n",
    "print(f'notre precision de classification {score}')\n",
    "# draw confusion matrix \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "array = confusion_matrix(labels, predictions, labels=['sports', 'cooking', 'travel'])\n",
    "df_cm = pd.DataFrame(array, index = ['sports', 'cooking', 'travel'],\n",
    "                  columns = ['pred_sports', 'pred_cooking', 'pred_travel'])\n",
    "\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche de Fichiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 results:\n",
      "modern_travel.docx\n",
      "supposed_travel.txt\n"
     ]
    }
   ],
   "source": [
    "def search_by_keyword():\n",
    "    keyword = input(\"Enter the keyword: \")\n",
    "    word =stemmer.stem(keyword.lower())\n",
    "    results = file_manager.search_keyword(word)\n",
    "    if len(results) == 0:\n",
    "        print(\"No results found\")\n",
    "    else:\n",
    "        print(f\"Found {len(results)} results:\")\n",
    "        for result in results:\n",
    "            print(result)\n",
    "\n",
    "search_by_keyword()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "sports\n",
      "sports\n",
      "travel\n",
      "travel\n",
      "travel\n",
      "travel\n",
      "Found 2 results:\n",
      "travelDoc.txt\n",
      "file2.txt\n"
     ]
    }
   ],
   "source": [
    "def search_by_keywords_association():\n",
    "    keywords = input(\"Enter the keywords separated by a comma: \")\n",
    "    results = file_manager.search_keyword_association(keywords)\n",
    "    if len(results) == 0:\n",
    "        print(\"No results found\")\n",
    "    else:\n",
    "        print(f\"Found {len(results)} results:\")\n",
    "        for result in results:\n",
    "            print(result)\n",
    "\n",
    "search_by_keywords_association()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modern_travel.docx', 'supposed_travel.txt', 'travelDoc.txt']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_in_content():\n",
    "    keyword = input(\"Enter the keyword: \")\n",
    "    results = []\n",
    "    for root, dirs, files in os.walk('docs'):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                file_content = file_manager.upload_file(file_path)\n",
    "                if keyword.lower() in file_content.lower():\n",
    "                        results.append(file_path.split(\"\\\\\")[-1])\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la lecture du fichier {file_path}: {e}\")\n",
    "    return results\n",
    "search_in_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation des critères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notre precision de classification 0.85\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_sports</th>\n",
       "      <th>pred_cooking</th>\n",
       "      <th>pred_travel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cooking</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred_sports  pred_cooking  pred_travel\n",
       "sports             5             0            2\n",
       "cooking            0             5            1\n",
       "travel             1             0            6"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path = \"dataset_for_query.csv\"\n",
    "\n",
    "# Read texts from the CSV dataset\n",
    "texts_without_labels, labels = read_texts_from_csv(csv_file_path)\n",
    "\n",
    "score = 0\n",
    "predictions = []\n",
    "i = 0\n",
    "for text in texts_without_labels:\n",
    "    max_sections = file_manager.search_keyword_association(text)\n",
    "    local_score = 0\n",
    "    for section in max_sections:\n",
    "        if file_manager.fileCategory[section] == labels[i]:\n",
    "            local_score += 1\n",
    "    local_score = local_score / len(max_sections)\n",
    "    if local_score >= 0.5:\n",
    "        score += 1\n",
    "    predictions.append(file_manager.fileCategory[max_sections[0]])\n",
    "    i += 1\n",
    "\n",
    "score = score / len(labels)\n",
    "print(f\"notre precision de classification {score}\")\n",
    "# draw confusion matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "array = confusion_matrix(labels, predictions, labels=[\"sports\", \"cooking\", \"travel\"])\n",
    "df_cm = pd.DataFrame(\n",
    "    array,\n",
    "    index=[\"sports\", \"cooking\", \"travel\"],\n",
    "    columns=[\"pred_sports\", \"pred_cooking\", \"pred_travel\"],\n",
    ")\n",
    "\n",
    "df_cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
